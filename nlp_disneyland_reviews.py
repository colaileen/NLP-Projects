# -*- coding: utf-8 -*-
"""NLP - Group Assignment - Group 2 - DisneyLand reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15I5z54rgC7MQy5uOxnLcdlR4SBovmYKn

##Exploring Disneyland Visitor Reviews: An NLP Approach

Natural Language Processing Group Project

Cecilia Prada, Jacobo Valderrama, Sofia Herrera, Valeria Castillo, Jacqueline Xu, Xuefei Wang, Juliana Lea√±o
"""

import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""##Data import from Kaggle"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("arushchillar/disneyland-reviews")

print("Path to dataset files:", path)

# Read the data
df = pd.read_csv(path+"/DisneylandReviews.csv",
                 delimiter=',', encoding='latin-1')

"""##Preprocessing: tokenization, lemmatization and stop words removal"""

#Based on Gordeliy, I. 2025. Frequency distribution and wordclouds. GitHub.
def preprocess_text(text):

    # Convert to lowercase
    text = str(text).lower()

    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenize
    tokens = word_tokenize(text)

    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))

    # Add custom stopwords relevant to Disneyland reviews
    stop_words |= set(['disney', 'disneyland', 'park', 'parks','u','ride','ha','went','get','got','wa','one','go','day','year','year old','old','hong','kong','hong kong','hk','paris','california'])
    tokens = [token for token in tokens if token not in stop_words]

    return ' '.join(tokens)

"""##Word clouds without bigrams"""

#Based on Gordeliy, I. 2025. Frequency distribution and wordclouds. GitHub.
def create_wordcloud(text, title):

    wordcloud = WordCloud(width=800, height=400,colormap='winter', background_color='white', prefer_horizontal=1.0).generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.tight_layout(pad=0)
    plt.savefig(f'wordcloud_{title.lower().replace(" ", "_")}.png')
    plt.show()

"""##Bigrams extraction and Word Clouds"""

#AI recommendations
from sklearn.feature_extraction.text import CountVectorizer # Extracts bigrams from text.
from wordcloud import WordCloud #Creates a word cloud visualization.
import matplotlib.pyplot as plt # Displays and saves the word cloud image

def create_wordcloud_bigrams(text, title):
    """Generates a word cloud with bigrams."""

    # Extract bigrams from text (2 word combinations)
    vectorizer = CountVectorizer(ngram_range=(2, 2))

    X = vectorizer.fit_transform([text])
    #Creates a sparse matrix where each row represents a document, and each column represents a bigram.

    bigram_freq = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))
    #Count Bigram Frequencies and stores it as dictionary

    # Generate word cloud from bigram frequencies
    wordcloud_bigrams = WordCloud(width=800, height=400, colormap='winter', background_color='white',
                          prefer_horizontal=1.0).generate_from_frequencies(bigram_freq)

    # Plot the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud_bigrams, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.tight_layout(pad=0)

    # Save word cloud as PNG
    plt.savefig(f'wordcloud_bigrams{title.lower().replace(" ", "_")}.png')
    plt.show()

"""##Plotting the frequency bar chart"""

#Based on Gordeliy, I. 2025. Frequency distribution and wordclouds. GitHub.
def plot_word_frequency(text, title, top_n=30):

    words = text.split()
    word_freq = Counter(words).most_common(top_n)

    words, freqs = zip(*word_freq)

    plt.figure(figsize=(12, 6))
    sns.barplot(x=list(freqs), y=list(words))
    plt.title(f'Top {top_n} Words - {title}')
    plt.xlabel('Frequency')
    plt.ylabel('Words')
    plt.tight_layout()
    plt.savefig(f'word_freq_{title.lower().replace(" ", "_")}.png')
    plt.show()

"""##Analyzing monthly trends and Plots

High rating = 5 stars and Low Rating=1 star
"""

#AI recommendations
def calculate_monthly_percentage(group_data, total_data, month):

    total_reviews = len(total_data[total_data['Month'] == month])
    if total_reviews == 0:
        return 0.0
    return (len(group_data) / total_reviews) * 100

def analyze_monthly_ratings_high(df, branch):

    df_copy = df.copy()

    # Extract month from Year_Month, handling missing values
    df_copy['Month'] = pd.to_numeric(
        df_copy['Year_Month'].str.split('-').str[1],
        errors='coerce'
    )

    # Drop rows with missing months
    df_copy = df_copy.dropna(subset=['Month'])
    df_copy['Month'] = df_copy['Month'].astype(int)

    # Filter for branch data first
    branch_data = df_copy[df_copy['Branch'] == branch]

    # Get high ratings for this branch
    high_ratings = branch_data[branch_data['Rating'].isin([5])]

    # Calculate monthly stats
    monthly_stats = []
    for month in range(1, 13):
        month_high_ratings = high_ratings[high_ratings['Month'] == month]
        month_total = branch_data[branch_data['Month'] == month]

        pct = calculate_monthly_percentage(month_high_ratings, branch_data, month)
        monthly_stats.append({
            'Month': month,
            'high_rating_pct': pct,
            'review_count': len(month_total)
        })

    monthly_stats = pd.DataFrame(monthly_stats)

    # Create the plot
    plt.figure(figsize=(12, 6))
    ax1 = plt.gca()
    ax2 = ax1.twinx()

    # Plot percentage line
    line1 = ax1.plot(monthly_stats['Month'], monthly_stats['high_rating_pct'],
                     color='green', marker='o', label='5-Star %')
    bars = ax2.bar(monthly_stats['Month'], monthly_stats['review_count'],
                   alpha=0.3, color='gray', label='Review Count')

    ax1.set_xlabel('Month')
    ax1.set_ylabel('Percentage of 5-Star Reviews', color='green')
    ax2.set_ylabel('Number of Reviews', color='gray')
    plt.xticks(range(1, 13))
    plt.title(f'High Ratings (5-Star) Analysis - {branch}')
    ax1.grid(True, linestyle='--', alpha=0.7)

    lines = line1 + [bars]
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper right')

    plt.tight_layout()
    plt.savefig(f'high_ratings_{branch.lower().replace(" ", "_")}.png')
    plt.show()

    print(f'\nHigh Ratings Analysis (5 Stars) for {branch}:')
    for _, row in monthly_stats.iterrows():
      print(f"Month {int(row['Month'])}: {row['high_rating_pct']:.1f}% 5-star reviews "
      f"(total reviews: {int(row['review_count'])})")


def analyze_monthly_ratings_low(df, branch):

    df_copy = df.copy()

    # Extract month from Year_Month, handling missing values
    df_copy['Month'] = pd.to_numeric(
        df_copy['Year_Month'].str.split('-').str[1],
        errors='coerce'
    )

    # Drop rows with missing months
    df_copy = df_copy.dropna(subset=['Month'])
    df_copy['Month'] = df_copy['Month'].astype(int)

    # Filter for branch data first
    branch_data = df_copy[df_copy['Branch'] == branch]

    # Get low ratings for this branch
    low_ratings = branch_data[branch_data['Rating'].isin([1])]

    # Calculate monthly stats
    monthly_stats = []
    for month in range(1, 13):
        month_low_ratings = low_ratings[low_ratings['Month'] == month]
        month_total = branch_data[branch_data['Month'] == month]

        pct = calculate_monthly_percentage(month_low_ratings, branch_data, month)
        monthly_stats.append({
            'Month': month,
            'low_rating_pct': pct,
            'review_count': len(month_total)
        })

    monthly_stats = pd.DataFrame(monthly_stats)

    # Create the plot
    plt.figure(figsize=(12, 6))
    ax1 = plt.gca()
    ax2 = ax1.twinx()

    # Plot percentage line
    line1 = ax1.plot(monthly_stats['Month'], monthly_stats['low_rating_pct'],
                     color='red', marker='o', label='1 Star %')
    bars = ax2.bar(monthly_stats['Month'], monthly_stats['review_count'],
                   alpha=0.3, color='gray', label='Review Count')

    ax1.set_xlabel('Month')
    ax1.set_ylabel('Percentage of 1 Star Reviews', color='red')
    ax2.set_ylabel('Number of Reviews', color='gray')
    plt.xticks(range(1, 13))
    plt.title(f'Low Ratings (1-Star) Analysis - {branch}')
    ax1.grid(True, linestyle='--', alpha=0.7)

    lines = line1 + [bars]
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper right')

    plt.tight_layout()
    plt.savefig(f'low_ratings_{branch.lower().replace(" ", "_")}.png')
    plt.show()

def calculate_monthly_percentage(group_data, total_data, month):

    total_reviews = len(total_data[total_data['Month'] == month])
    if total_reviews == 0:
        return 0.0
    return (len(group_data) / total_reviews) * 100

def analyze_monthly_ratings_high(df, branch):

    df_copy = df.copy()

    # Extract month from Year_Month, handling missing values
    df_copy['Month'] = pd.to_numeric(
        df_copy['Year_Month'].str.split('-').str[1],
        errors='coerce'
    )

    # Drop rows with missing months
    df_copy = df_copy.dropna(subset=['Month'])
    df_copy['Month'] = df_copy['Month'].astype(int)

    # Filter for branch data first
    branch_data = df_copy[df_copy['Branch'] == branch]

    # Get high ratings for this branch
    high_ratings = branch_data[branch_data['Rating'].isin([5])]

    # Calculate monthly stats
    monthly_stats = []
    for month in range(1, 13):
        month_high_ratings = high_ratings[high_ratings['Month'] == month]
        month_total = branch_data[branch_data['Month'] == month]

        pct = calculate_monthly_percentage(month_high_ratings, branch_data, month)
        monthly_stats.append({
            'Month': month,
            'high_rating_pct': pct,
            'review_count': len(month_total)
        })

    monthly_stats = pd.DataFrame(monthly_stats)

    # Create the plot
    plt.figure(figsize=(12, 6))
    ax1 = plt.gca()
    ax2 = ax1.twinx()

    # Plot percentage line
    line1 = ax1.plot(monthly_stats['Month'], monthly_stats['high_rating_pct'],
                     color='green', marker='o', label='5-Star %')
    bars = ax2.bar(monthly_stats['Month'], monthly_stats['review_count'],
                   alpha=0.3, color='gray', label='Review Count')

    ax1.set_xlabel('Month')
    ax1.set_ylabel('Percentage of 5-Star Reviews', color='green')
    ax2.set_ylabel('Number of Reviews', color='gray')
    plt.xticks(range(1, 13))
    plt.title(f'High Ratings (5-Star) Analysis - {branch}')
    ax1.grid(True, linestyle='--', alpha=0.7)

    lines = line1 + [bars]
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper right')

    plt.tight_layout()
    plt.savefig(f'high_ratings_{branch.lower().replace(" ", "_")}.png')
    plt.show()

    print(f'\nHigh Ratings Analysis (5 Stars) for {branch}:')
    for _, row in monthly_stats.iterrows():
      print(f"Month {int(row['Month'])}: {row['high_rating_pct']:.1f}% 5-star reviews "
      f"(total reviews: {int(row['review_count'])})")


def analyze_monthly_ratings_low(df, branch):

    df_copy = df.copy()

    # Extract month from Year_Month, handling missing values
    df_copy['Month'] = pd.to_numeric(
        df_copy['Year_Month'].str.split('-').str[1],
        errors='coerce'
    )

    # Drop rows with missing months
    df_copy = df_copy.dropna(subset=['Month'])
    df_copy['Month'] = df_copy['Month'].astype(int)

    # Filter for branch data first
    branch_data = df_copy[df_copy['Branch'] == branch]

    # Get low ratings for this branch
    low_ratings = branch_data[branch_data['Rating'].isin([1])]

    # Calculate monthly stats
    monthly_stats = []
    for month in range(1, 13):
        month_low_ratings = low_ratings[low_ratings['Month'] == month]
        month_total = branch_data[branch_data['Month'] == month]

        pct = calculate_monthly_percentage(month_low_ratings, branch_data, month)
        monthly_stats.append({
            'Month': month,
            'low_rating_pct': pct,
            'review_count': len(month_total)
        })

    monthly_stats = pd.DataFrame(monthly_stats)

    # Create the plot
    plt.figure(figsize=(12, 6))
    ax1 = plt.gca()
    ax2 = ax1.twinx()

    # Plot percentage line
    line1 = ax1.plot(monthly_stats['Month'], monthly_stats['low_rating_pct'],
                     color='red', marker='o', label='1-Star %')
    bars = ax2.bar(monthly_stats['Month'], monthly_stats['review_count'],
                   alpha=0.3, color='gray', label='Review Count')

    ax1.set_xlabel('Month')
    ax1.set_ylabel('Percentage of 1 Star Reviews', color='red')
    ax2.set_ylabel('Number of Reviews', color='gray')
    plt.xticks(range(1, 13))
    plt.title(f'Low Ratings (1-Star) Analysis - {branch}')
    ax1.grid(True, linestyle='--', alpha=0.7)

    lines = line1 + [bars]
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper right')

    plt.tight_layout()
    plt.savefig(f'low_ratings_{branch.lower().replace(" ", "_")}.png')
    plt.show()

    print(f"\nLow Ratings Analysis (1-Star) for {branch}:")
    for _, row in monthly_stats.iterrows():
        print(f'Month {int(row["Month"])}: {row["low_rating_pct"]:.1f}% 1-star reviews'
              f'(total reviews: {int(row["review_count"])})')

"""Analyzing the text data set

"""

nltk.download('punkt_tab')

def main():

    print(df.describe()) #summary statistics of the dataset

    # Preprocess reviews
    df['processed_text'] = df['Review_Text'].apply(preprocess_text)

    # Analyze for each branch and rating combination
    branches = df['Branch'].unique()
    ratings = [1, 5]  # We only want 1-star and 5-star reviews

    # Print summary statistics
    print('\nSummary Statistics:')
    for branch in branches:
        print(f"\n{branch}:")
        total = len(df[(df['Branch'] == branch)])
        print(f'total reviews : {total}')
        for rating in ratings:
            count = len(df[(df['Branch'] == branch) & (df['Rating'] == rating)])
            percentage = (count/total)*100
            print(f'{rating}-star reviews: {count} ({percentage:.1f}% of total reviews)')

    # First, perform the monthly rating analysis for each branch
    for branch in branches:
        analyze_monthly_ratings_high(df, branch)
        analyze_monthly_ratings_low(df, branch)

    # Then perform the word analysis
    for branch in branches:
        for rating in ratings:
            # Filter data
            mask = (df['Branch'] == branch) & (df['Rating'] == rating)
            reviews = df[mask]['processed_text']

            if len(reviews) > 0:
                # Combine all reviews for this branch and rating
                combined_text = ' '.join(reviews)

                # Create title for plots
                title = f'{branch} {rating}-Star Reviews'

                # Generate wordcloud
                create_wordcloud(combined_text, title)
                create_wordcloud_bigrams(combined_text, title)

                # Create frequency distribution plot
                plot_word_frequency(combined_text, title)


if __name__ == "__main__":
    main()

"""##Vectorization and topic modeling"""

# built-in Python libraries
# -------------------------
import collections
import re
import string
import warnings
warnings.filterwarnings('ignore')


!pip install -U gensim

# Gensim for topic modeling
import gensim
# for loading data
import sklearn.datasets
# for LDA visualization
!pip install pyLDAvis
import pyLDAvis
import pyLDAvis.gensim_models

# for uploading data files
from google.colab import files


#REVIEW IF DUPLICATED
def text_to_lemma_frequencies(text, remove_stop_words=True):

    # split document into sentences
    sentences = nltk.sent_tokenize(text)

    # create a place to store (word, pos_tag) tuples
    words_and_pos_tags = []

    # get all words and pos tags
    for sentence in sentences:
        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))

    # load the lemmatizer
    lemmatizer = nltk.stem.WordNetLemmatizer()

    # lemmatize the words
    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \
              (word,pos) in words_and_pos_tags]

    # convert to lowercase
    lowercase_lemmas = [lemma.lower() for lemma in lemmas]

    # load the stopword list for English
    stop_words = set([])
    if remove_stop_words:
        stop_words = set(nltk.corpus.stopwords.words('english'))

    # add punctuation to the set of things to remove
    all_removal_tokens = stop_words | set(string.punctuation)

    # bonus: also add some custom double-quote tokens to this set
    all_removal_tokens |= set(["''","``"])

    # only get lemmas that aren't in these lists
    content_lemmas = [lemma for lemma in lowercase_lemmas \
                      if lemma not in all_removal_tokens and \
                      re.match(r"^\w+$",lemma)]

    # return the frequency distribution object
    return nltk.probability.FreqDist(content_lemmas)

def docs2matrix(document_list):

    # use the vocab2index idea from before
    vocab2index = {}

    # load the stopword list for English
    stop_words = set(nltk.corpus.stopwords.words('english'))
    stop_words |= set(['from', 'subject', 're', 'edu', 'use'])

    # add punctuation to the set of things to remove
    all_removal_tokens = stop_words | set(string.punctuation)

    # bonus: also add some custom double-quote tokens to this set
    all_removal_tokens |= set(["''","``"])

    vocab2index = {}
    latest_index = 0

    lfs = []
    # this should be a nice starting point
    for doc in document_list:
        lf = text_to_lemma_frequencies(doc,all_removal_tokens)
        for token in lf.keys():
            if token not in vocab2index:
                vocab2index[token] = latest_index
                latest_index += 1

        lfs.append(lf)

    #create the zeros matrix
    corpus_matrix = np.zeros((len(lfs), len(vocab2index)))

    for row, lf in enumerate(lfs):
        for token, frequency in lf.items():
            column = vocab2index[token]
            corpus_matrix[row][column] = frequency

    return corpus_matrix, vocab2index


# Lemmatization -- redefining this here to make
# code block more self-contained
def lookup_pos(pos):
    pos_first_char = pos[0].lower()
    if pos_first_char in 'nv':
        return pos_first_char
    else:
        return 'n'

print()
print("Done with setup!")
print("If you'd like, you can click the (X) button to the left to clear this output.")

import numpy as np
from gensim import corpora
import nltk
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')
# Download NLTK stopwords
nltk.download('punkt')
nltk.download('stopwords')

import numpy as np
import pandas as pd
import kagglehub
from gensim import corpora
import nltk

nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

# Download latest version of the dataset
path = kagglehub.dataset_download("arushchillar/disneyland-reviews")
print("Path to dataset files:", path)

# Load the dataset
df = pd.read_csv(path+"/DisneylandReviews.csv", delimiter=',', encoding='latin-1')

# Check the first few rows
print(df.head())

#Dataset preparation for topic modeling for Rating=1 reviews

filtered_reviews_1_all = df[(df['Rating'] == 1)].copy()
filtered_reviews_1_all

import nltk
nltk.download('wordnet')
# Select the text column for analysis
documents = filtered_reviews_1_all['Review_Text'].dropna().tolist()  # Drop missing reviews


# Download NLTK stopwords
import nltk
from nltk.corpus import stopwords # Import stopwords module
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))

# Function to preprocess text
def preprocess(text):
    tokens = word_tokenize(text.lower())  # Tokenize and lowercase
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Remove stopwords & non-words
    return tokens

# Preprocess all reviews
processed_docs = [preprocess(doc) for doc in documents]

# Create a Gensim dictionary
dictionary = corpora.Dictionary(processed_docs)

# Convert documents to Bag-of-Words (BoW)
corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

# using the function we wrote before, but modified to also return the vocab2index
corpus_matrix, word2id = docs2matrix(documents)
# reverse this dictionary
id2word = {v:k for k,v in word2id.items()}

corpus = gensim.matutils.Dense2Corpus(corpus_matrix, documents_columns=False)
print("Loaded",len(corpus),"documents into a Gensim corpus.")

warnings.filterwarnings('ignore')


# run LDA on our corpus, using out dictionary (k=6)
lda = gensim.models.LdaModel(corpus, id2word=id2word, num_topics=6, passes = 10, iterations = 200, random_state=42)
lda.print_topics()

"""Deleting additional words to make facilitate topic identification"""

import numpy as np
import gensim

# Compute total word frequencies
total_counts = np.sum(corpus_matrix, axis=0)

# Sort words by frequency
sorted_words = sorted(zip(range(len(total_counts)), total_counts), key=lambda x: x[1], reverse=True)

# Define thresholds
N = 100  # Remove top N most frequent words
M = 50   # Remove words appearing less than M times

# Get indexes of words to remove
top_N_ids = [item[0] for item in sorted_words[:N]]
appears_less_than_M_times = [item[0] for item in sorted_words if item[1] < M]

# Ensure that `vocab_dense` only contains actual words
vocab_dense = [id2word[idx] for idx in range(len(id2word)) if id2word[idx].isalpha()]  # Keep only words

print("Top words to remove:", ' '.join([id2word[idx] for idx in top_N_ids if idx < len(vocab_dense)]))

# Ensure all indexes are valid
remove_indexes = [idx for idx in (top_N_ids + appears_less_than_M_times) if idx < len(vocab_dense)]
remove_indexes = sorted(set(remove_indexes), reverse=True)  # Remove duplicates and sort descending

# Remove words safely
for index in remove_indexes:
    if index < len(vocab_dense):
        del vocab_dense[index]

# Ensure valid column removal from corpus_matrix
remove_indexes = [idx for idx in remove_indexes if idx < corpus_matrix.shape[1]]
corpus_matrix_filtered = np.delete(corpus_matrix, remove_indexes, axis=1)

# Update id2word and word2id dictionaries
id2word_filtered = {i: word for i, word in enumerate(vocab_dense)}
word2id_filtered = {word: i for i, word in id2word_filtered.items()}

# Convert to Gensim Corpus
corpus_filtered = gensim.matutils.Dense2Corpus(corpus_matrix_filtered, documents_columns=False)

print("Original matrix shape:", corpus_matrix.shape)
print("New matrix shape:", corpus_matrix_filtered.shape)

import gensim

# Recreating dictionary
dictionary_filtered = gensim.corpora.Dictionary(processed_docs)
dictionary_filtered.filter_extremes(no_below=M, no_above=1.0, keep_n=None) # This is a method to remove words based on your frequency thresholds (N and M)

# Converting filtered documents to BoW using the filtered dictionary
corpus_filtered = [dictionary_filtered.doc2bow(doc) for doc in processed_docs]

# Updated id2word from the filtered dictionary
id2word_filtered = dictionary_filtered

# Traiing LDA model using the filtered corpus and updated dictionary
lda = gensim.models.LdaModel(corpus_filtered,
                            id2word=id2word_filtered,
                            num_topics=6,
                            passes=10,
                            iterations=200,
                            random_state=200)

lda.print_topics()

import pandas as pd
import kagglehub

# Download latest version
path = kagglehub.dataset_download("arushchillar/disneyland-reviews")

print("Path to dataset files:", path)

# Read the data
df = pd.read_csv(path+"/DisneylandReviews.csv",
                 delimiter=',', encoding='latin-1')

import gensim
from sklearn.feature_extraction.text import TfidfTransformer
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re


import gensim
from sklearn.feature_extraction.text import TfidfTransformer

# Compute TF-IDF transformation
tfidf_transformer = TfidfTransformer()
corpus_tfidf = tfidf_transformer.fit_transform(corpus_matrix)  # Apply TF-IDF

# Convert sparse matrix to NumPy array
tfidf_array = corpus_tfidf.toarray()

# Compute total word importance (average TF-IDF per word)
average_tfidf = np.mean(tfidf_array, axis=0)  # Average TF-IDF score per word across all documents

# Compute document frequency (how many documents contain each word)
doc_freq = np.sum(corpus_matrix > 0, axis=0).flatten()  # Number of documents in which each word appears

# Sort words by TF-IDF importance (descending)
sorted_words = sorted(zip(range(len(average_tfidf)), average_tfidf), key=lambda x: x[1], reverse=True)

# Define thresholds
N = 100   # Remove top N most important words (highest average TF-IDF)
M = 50  # Remove words appearing in fewer than M documents

# Get indexes of words to remove
top_N_ids = [item[0] for item in sorted_words[:N]]  # Top N based on highest TF-IDF scores
low_doc_freq_ids = [idx for idx in range(len(doc_freq)) if doc_freq[idx] < M]  # Low-frequency words

# Ensure that `vocab_dense` only contains actual words (no special chars or stopwords)
vocab_dense = [id2word[idx] for idx in range(len(id2word)) if id2word[idx].isalpha()]  # Keep only valid words

# Print top words being removed (those with high TF-IDF)
print("Top words to remove (high TF-IDF):", ' '.join([id2word[idx] for idx in top_N_ids if idx < len(vocab_dense)]))

# Ensure valid indexes to remove
remove_indexes = sorted(set(top_N_ids + low_doc_freq_ids), reverse=True)  # Remove duplicates and sort

# Remove words safely from `vocab_dense`
for index in remove_indexes:
    if index < len(vocab_dense):
        del vocab_dense[index]

# Ensure valid column removal from corpus_matrix
remove_indexes = [idx for idx in remove_indexes if idx < corpus_matrix.shape[1]]
corpus_matrix_filtered = np.delete(tfidf_array, remove_indexes, axis=1)  # Remove corresponding columns

# Update id2word and word2id dictionaries
id2word_filtered = {i: word for i, word in enumerate(vocab_dense)}
word2id_filtered = {word: i for i, word in id2word_filtered.items()}

# Convert to Gensim Corpus (after filtering)
corpus_filtered_TF_IDF = gensim.matutils.Dense2Corpus(corpus_matrix_filtered, documents_columns=False)

print("Original matrix shape:", corpus_matrix.shape)
print("TF-IDF matrix shape:", corpus_tfidf.shape)
print("Filtered matrix shape:", corpus_matrix_filtered.shape)

import gensim

# Recreating dictionary
dictionary_filtered = gensim.corpora.Dictionary(processed_docs)
dictionary_filtered.filter_extremes(no_below=M, no_above=1.0, keep_n=None) # This is a method to remove words based on your frequency thresholds (N and M)

# Converting filtered documents to BoW using the filtered dictionary
corpus_filtered_TF_IDF = [dictionary_filtered.doc2bow(doc) for doc in processed_docs]

# Updated id2word from the filtered dictionary
id2word_filtered = dictionary_filtered

# Traiing LDA model using the filtered corpus and updated dictionary
lda = gensim.models.LdaModel(corpus_filtered_TF_IDF,
                            id2word=id2word_filtered,
                            num_topics=6,
                            passes=10,
                            iterations=200,
                            random_state=200)

lda.print_topics()

!pip install pyLDAvis
import pyLDAvis
import gensim

pyLDAvis.enable_notebook()


dictionary = gensim.corpora.Dictionary() #genism dictionary created as this is what pyLDA expects as input
dictionary.token2id = word2id_filtered

# visualize the LDA model
vis = pyLDAvis.gensim_models.prepare(lda, corpus_filtered, dictionary_filtered)
vis

"""##Sentiment analysis

Google's GoEmotions
"""

df2=df.copy()
filtered_reviews_1_Paris = df2[(df2['Rating'] == 1) & (df2['Branch'] == 'Disneyland_Paris')].copy()
filtered_reviews_1_Paris

#Preprocessing filtered text 1 star - Paris
import nltk
nltk.download('punkt_tab')  # Download punkt_tab here
#filtered_reviews_1_Paris['Review_Text'].apply(preprocess_text)

from transformers import AutoTokenizer
import pandas as pd

# Load the GoEmotions tokenizer (based on BERT)
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
# This tokenizer breaks text into tokens that the model can understand.

# Function to split long reviews into chunks of a maximum of 512 tokens
def split_long_reviews(text, max_length=512):
    tokens = tokenizer.encode(text, truncation=False)  # Tokenize without truncating
    chunks = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]
    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]

# BERT-based models can only process 512 tokens at a time.
# If a review is longer than 512 tokens, it needs to be split into smaller chunks.

# Apply the function to each review and flatten the list
processed_reviews_1_Paris = filtered_reviews_1_Paris['Review_Text'].apply(lambda x: split_long_reviews(x)).explode().tolist()

# Verify that long reviews were correctly split
print(f"Total number of fragments to analyze: {len(processed_reviews_1_Paris)}")
print(f"Example of a fragment:\n{processed_reviews_1_Paris[0]}")

#Load the GoEmotions Model & Classify Emotions (Davidson, 2024)
from transformers import pipeline

emotion_classifier = pipeline("text-classification",
                              model="joeddav/distilbert-base-uncased-go-emotions-student",truncation=True,
                              top_k=None)
# Aplicar GoEmotions, Runs the classifier on all processed reviews (all_reviews).
emotions_1_Paris = emotion_classifier(processed_reviews_1_Paris)

# prints detected emotions
print(emotions_1_Paris)

# Dictionary to store total score and count for each emotion
emotion_data = {}

for review_emotions in emotions_1_Paris:
    for emotion in review_emotions:
        label = emotion['label']
        score = emotion['score']
        if label in emotion_data:
            emotion_data[label]["total_score"] += score
            emotion_data[label]["count"] += 1
        else:
            emotion_data[label] = {"total_score": score, "count": 1}

# Convert to DataFrame and calculate average
emotion_1_Paris_df = pd.DataFrame([
    {"Emotion": label,
     "Total Score": data["total_score"],
     "Average Score": data["total_score"] / data["count"]}
    for label, data in emotion_data.items()
])

# Sort by total score in descending order
emotion_1_Paris_df = emotion_1_Paris_df.sort_values(by="Total Score", ascending=False)

# Print formatted table
print("\nüîπ Emotion Distribution in Disneyland Paris Negative Reviews:")
print(emotion_1_Paris_df.to_string(index=False))  # Print table without row index

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# Convert the results into a flat list of emotions
emotion_counts = Counter()

for review in emotions_1_Paris:  # Iterate over each review
    for emotion in review:  # Iterate over each detected emotion in the review
        emotion_counts[emotion['label']] += emotion['score']  # Add up the score of each emotion

# Sort emotions from highest to lowest total score
sorted_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)

# Convert to separate lists
labels, scores = zip(*sorted_emotions)

# Create the sorted bar chart
plt.figure(figsize=(12, 6))
sns.barplot(x=list(labels), y=list(scores), palette="viridis")

plt.xticks(rotation=90)
plt.xlabel("Emotion")
plt.ylabel("Total Score")
plt.title("Distribution of Emotions in Paris Reviews")
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Set a threshold (e.g., group emotions with <5% into "Others")
threshold = 3  # Adjust as needed
total = sum(emotion_1_Paris_df["Average Score"])  # Total sum of averages

# Create a new DataFrame for grouping
filtered_df = emotion_1_Paris_df.copy()
filtered_df["Percentage"] = (filtered_df["Average Score"] / total) * 100

# Separate emotions above and below the threshold
main_emotions = filtered_df[filtered_df["Percentage"] >= threshold]
others = filtered_df[filtered_df["Percentage"] < threshold]

# Sum the "Others" category
if not others.empty:
    others_row = pd.DataFrame([{"Emotion": "Others", "Average Score": others["Average Score"].sum(), "Percentage": others["Percentage"].sum()}])
    main_emotions = pd.concat([main_emotions, others_row])

# Extract labels and values
labels = main_emotions["Emotion"]
sizes = main_emotions["Average Score"]

# Plot the donut chart
plt.figure(figsize=(8, 8))
colors = sns.color_palette("viridis", len(labels))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors, wedgeprops={'edgecolor': 'white'})

# Add a white center circle for the donut effect
center_circle = plt.Circle((0, 0), 0.70, fc='white')
plt.gca().add_artist(center_circle)

# Title
plt.title("Average Score Distribution of Emotions (Disneyland Paris)")

# Show the chart
plt.show()

#Full sentiment analysis to analyze negative reviews in California
df4=df.copy()
filtered_reviews_1_California = df4[(df4['Rating'] == 1) & (df4['Branch'] == 'Disneyland_California')].copy()
filtered_reviews_1_California

# Load the GoEmotions tokenizer (based on BERT)
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
# This tokenizer breaks text into tokens that the model can understand.

# BERT-based models can only process 512 tokens at a time.
# If a review is longer than 512 tokens, it needs to be split into smaller chunks.

# Apply the function to each review and flatten the list
filtered_reviews_1_California = filtered_reviews_1_California['Review_Text'].apply(lambda x: split_long_reviews(x)).explode().tolist()

# Verify that long reviews were correctly split
print(f"Total number of fragments to analyze: {len(filtered_reviews_1_California)}")
print(f"Example of a fragment:\n{filtered_reviews_1_California[0]}")


emotion_classifier = pipeline("text-classification",
                              model="joeddav/distilbert-base-uncased-go-emotions-student",truncation=True,
                              top_k=None)
# Aply GoEmotions, Runs the classifier on all processed reviews (all_reviews).
emotions_1_California = emotion_classifier(filtered_reviews_1_California)

# prints detected emotions
print(emotions_1_California)

# Convert the results into a flat list of emotions
emotion_counts = Counter()

for review in emotions_1_California:
    for emotion in review:
        emotion_counts[emotion['label']] += emotion['score']

# Sort emotions from highest to lowest total score
sorted_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)

# Convert to separate lists
labels, scores = zip(*sorted_emotions)

# Create the sorted bar chart
plt.figure(figsize=(12, 6))
sns.barplot(x=list(labels), y=list(scores), palette="viridis")

plt.xticks(rotation=90)
plt.xlabel("Emotion")
plt.ylabel("Total Score")
plt.title("Distribution of emotions California")
plt.show()

#Full sentiment analysis to analyze negative reviews in other countries
df5=df.copy()
filtered_reviews_1_HongKong = df5[(df5['Rating'] == 1) & (df5['Branch'] == 'Disneyland_HongKong')].copy()
filtered_reviews_1_HongKong


# Load the GoEmotions tokenizer (based on BERT)
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
# This tokenizer breaks text into tokens that the model can understand.

# BERT-based models can only process 512 tokens at a time.
# If a review is longer than 512 tokens, it needs to be split into smaller chunks.

# Apply the function to each review and flatten the list
filtered_reviews_1_HongKong = filtered_reviews_1_HongKong['Review_Text'].apply(lambda x: split_long_reviews(x)).explode().tolist()

# Verify that long reviews were correctly split
print(f"Total number of fragments to analyze: {len(filtered_reviews_1_HongKong)}")
print(f"Example of a fragment:\n{filtered_reviews_1_HongKong[0]}")


emotion_classifier = pipeline("text-classification",
                              model="joeddav/distilbert-base-uncased-go-emotions-student",truncation=True,
                              top_k=None)
# Aply GoEmotions, Runs the classifier on all processed reviews (all_reviews).
emotions_1_HongKong = emotion_classifier(filtered_reviews_1_HongKong)

# prints detected emotions
print(emotions_1_HongKong)

# Convert the results into a flat list of emotions
emotion_counts = Counter()

for review in emotions_1_HongKong:  # Iterate over each review
    for emotion in review:  # Iterate over each detected emotion in the review
        emotion_counts[emotion['label']] += emotion['score']  # Sum the score of each emotion

# Sort emotions from highest to lowest total score
sorted_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)

# Convert to separate lists
labels, scores = zip(*sorted_emotions)

# Create the sorted bar chart
plt.figure(figsize=(12, 6))
sns.barplot(x=list(labels), y=list(scores), palette="viridis")

plt.xticks(rotation=90)
plt.xlabel("Emotion")
plt.ylabel("Total Score")
plt.title("Distribution of Emotions in Hong Kong")
plt.show()